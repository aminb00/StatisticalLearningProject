{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5263a-0e62-4b8b-8a26-e0cbf7437865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizzazione\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns\n",
    "\n",
    "# Modelli statistici\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Libreria ISLP (Statistical Learning)\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS ,summarize, poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ab82c-9994-4616-b7ec-57f76b896520",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "We will use the Boston housing dataset, contained in the ISLP package. The Boston dataset records the medv (average house value) for 506 neighbourhoods around Boston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b89e1-e975-4fdf-93d7-06109a04c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carico il dataset e stampo l'head\n",
    "f1 = pd.read_csv(\"f1_pitstops_2018_2024.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329c71f-5309-4986-bbf4-99e3f155d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f4a7c-e0b1-4392-bd94-b60fe9e52659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the first 5 rows in the datasets\n",
    "f1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57595b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05142c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controllo i dati nulli\n",
    "columnsWithNulls=f1.isnull().sum().sort_values(ascending=False)\n",
    "columnsWithNulls=columnsWithNulls[columnsWithNulls>0]\n",
    "print(columnsWithNulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controllo in percentuale quanti dati mancano così da capire come trattarli, pongo una soglia di eliminazione del regressore nel caso di +30% di dati mancanti\n",
    "missing_pct = f1.isnull().mean() * 100\n",
    "missing_only = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "print(missing_only)\n",
    "sns.barplot(x=missing_only.values, y=missing_only.index)\n",
    "plt.title('Percentuale di valori nulli per colonna')\n",
    "plt.xlabel('% Missing')\n",
    "plt.ylabel('Colonna')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droppiamo le righe con i valori nulli e le colonne con più del 30% di dati nulli\n",
    "f1.dropna(axis=0, inplace=True)\n",
    "f1.drop(columns=missing_only[missing_only>30].index, inplace=True)\n",
    "f1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba6573",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3400abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleziona le colonne numeriche\n",
    "numeric_f1 = f1.select_dtypes(include=[np.number])\n",
    "\n",
    "# Controllo che ci siano almeno 4 variabili numeriche per evitare visualizzazioni poco utili\n",
    "if numeric_f1.shape[1] >= 4:\n",
    "    # Calcola la matrice di correlazione\n",
    "    corr = numeric_f1.corr()\n",
    "    \n",
    "    # Imposta la figura\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='coolwarm',\n",
    "        square=True,\n",
    "        linewidths=.5,\n",
    "        cbar_kws={\"shrink\": .75}\n",
    "    )\n",
    "    plt.title('Correlation Heatmap of Numeric Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Not enough numeric columns for a meaningful correlation heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Position Changes', 'Total Pit Stops', 'Fast Lap Attempts', 'Lap Time Variation', 'Air_Temp_C', 'AvgPitStopTime']\n",
    "f1.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d565ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleziona le colonne numeriche\n",
    "numeric_f1 = f1.select_dtypes(include=[np.number])\n",
    "\n",
    "# Controllo che ci siano almeno 4 variabili numeriche per evitare visualizzazioni poco utili\n",
    "if numeric_f1.shape[1] >= 4:\n",
    "    # Calcola la matrice di correlazione\n",
    "    corr = numeric_f1.corr()\n",
    "    \n",
    "    # Imposta la figura\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='coolwarm',\n",
    "        square=True,\n",
    "        linewidths=.5,\n",
    "        cbar_kws={\"shrink\": .75}\n",
    "    )\n",
    "    plt.title('Correlation Heatmap of Numeric Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Not enough numeric columns for a meaningful correlation heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d29fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = f1.select_dtypes(include='number').columns\n",
    "x_vars = [col for col in numeric_cols if col != 'Tire Usage Aggression']\n",
    "\n",
    "# Controlla se ci sono abbastanza variabili da visualizzare\n",
    "n = len(x_vars)\n",
    "if n > 0:\n",
    "    # Imposta il layout della figura\n",
    "    fig, axes = plt.subplots(1, n, figsize=(5 * n, 5), sharey=True)\n",
    "\n",
    "    # Scatterplot di ogni variabile rispetto al Driver Aggression Score\n",
    "    for i, var in enumerate(x_vars):\n",
    "        sns.scatterplot(data=f1, x=var, y='Tire Usage Aggression', ax=axes[i])\n",
    "        axes[i].set_title(f'{var} vs Aggression')\n",
    "        axes[i].set_xlabel(var)\n",
    "\n",
    "    axes[0].set_ylabel('Tire Usage Aggression')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Nessuna variabile numerica disponibile per il confronto con Driver Aggression Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea978a7b",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc12625",
   "metadata": {},
   "source": [
    "### Choose the first 3 variables that are more promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "target = 'Tire Usage Aggression'\n",
    "\n",
    "results = []\n",
    "\n",
    "# Looping every possible variable\n",
    "for col in numeric_cols:\n",
    "    X = sm.add_constant(f1[[col]])\n",
    "    y = f1[target]\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    results.append({\n",
    "        'variable': col,\n",
    "        'r_squared': model.rsquared,\n",
    "        'p_value': model.pvalues[col]\n",
    "    })\n",
    "\n",
    "# Transforming data frame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ordering\n",
    "top_r2_pval = results_df.sort_values(by='p_value', ascending=True).head(4)\n",
    "worse_r2_pval = results_df.sort_values(by='p_value', ascending=False).head(3)\n",
    "\n",
    "\n",
    "# Output\n",
    "print(\"VARIABILI CON R² PIU' ALTO E p-value PIU BASSO:\")\n",
    "print(top_r2_pval)\n",
    "\n",
    "print(\"\\n\\nVARIABILI CON R² PIU' BASSO E p-value PIU ALTO:\\n\")\n",
    "print(worse_r2_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1250b-ad50-4afe-8955-8593aca616e4",
   "metadata": {},
   "source": [
    "### Fit Linear Model: mdev = b0 + b1*TotalPitStops + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818a55c-e679-452e-b6f9-2034a0096bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame({'intercept': np.ones(f1.shape[0]), 'TotalPitStops': f1['TotalPitStops']})\n",
    "X[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d573f84-762b-4e5b-b69b-2de6bff54276",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f1[target]\n",
    "model = sm.OLS(y, X) #function to fit a simple linear regression. Here we define the model\n",
    "results = model.fit() #here we fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80353953-8996-45b8-8449-b110d7d73729",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())\n",
    "\n",
    "significant_vars = results.pvalues[results.pvalues < 0.05]\n",
    "print(\"\\nVariabili significative (p-value < 0.05):\")\n",
    "print(significant_vars)\n",
    "\n",
    "insignificant_vars = results.pvalues[results.pvalues >= 0.05]\n",
    "print(\"\\nVariabili insignificanti (p-value >= 0.05):\")\n",
    "print(insignificant_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e963f-1ef0-48b8-9d2e-86ff67042753",
   "metadata": {},
   "source": [
    "### Creating the input matrix using ModelSpec of ISLP package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6870c97-b145-4fa2-92e3-9aa8a6c861c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MS(['TotalPitStops'])\n",
    "model = model.fit(f1) \n",
    "X = model.transform(f1)\n",
    "X[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4434ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y, kde=True)\n",
    "\n",
    "plt.xlim(y.quantile(0.00), y.quantile(0.95))\n",
    "\n",
    "plt.title('Distribution of Tire Usage Aggression (Up to 95th Percentile)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(f1['TotalPitStops'], kde=True)\n",
    "\n",
    "plt.xticks(f1['TotalPitStops'].unique(), rotation=0)\n",
    "\n",
    "plt.title('Distribution of Total Pit Stops')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d15b7-26b8-404d-9405-8f6c1b5752dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on new input\n",
    "new_df = pd.DataFrame({'TotalPitStops': [2, 3, 4, 5, 6, 7]})\n",
    "new_X = model.transform(new_df)  # Aggiungi una colonna di 1 per l'intercetta\n",
    "new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aee013",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_X = sm.add_constant(new_df)\n",
    "new_predictions = results.get_prediction(new_X)\n",
    "predicted_means = new_predictions.predicted_mean\n",
    "print(predicted_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be302f-e3c9-413a-b530-6fdd7b4649d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confidence interval\n",
    "new_predictions.conf_int(alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26905a43-9ed6-402d-bfc8-151ccb3cf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = results.params.iloc[0]  # Intercetta (b)\n",
    "slope = results.params.iloc[1]  # Pendenza (m)\n",
    "\n",
    "formula = f\"y = {slope:.4f} * TotalPitStops + {intercept:.4f}\"\n",
    "\n",
    "print(\"FORMULA del modello di regressione:\", formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(ax, b, m, *args, **kwargs):\n",
    "    \"Aggiungi una retta con pendenza m e intercetta b su ax\"\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n",
    "    ax.plot(xlim, ylim, *args, **kwargs)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.scatter(f1['TotalPitStops'], f1['Tire Usage Aggression'], alpha=0.5)\n",
    "\n",
    "abline(ax, intercept, slope, 'r--', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603269ac-2cbd-444a-ab32-a147df63343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics plot (We observe non linearity)\n",
    "fig, ax = plt.subplots(figsize=(8, 8)) \n",
    "ax.scatter(results.fittedvalues, results.resid)  # Scatter plot of fitted values vs residuals\n",
    "ax.set_xlabel('Fitted value')\n",
    "ax.set_ylabel('Residual')\n",
    "ax.axhline(0, c='k', ls='--')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408506cc-aa9f-429e-9ca3-80d1b2170d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute leverage statistics\n",
    "infl = results.get_influence()\n",
    "ax = plt.subplots(figsize=(8,8))[1]\n",
    "ax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Leverage')\n",
    "np.argmax(infl.hat_matrix_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f69d30-324d-4f5a-a24e-0db883a48c29",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdc0aa-afdf-4b3e-8df9-38f3564fd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = b*X + e (perform a regression using all of the predictors)\n",
    "terms = numeric_cols.drop('Tire Usage Aggression')\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad2ed9-67f6-4e4b-9e7b-fdbc1b33180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MS(terms).fit_transform(f1)\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c44b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_vars = results.pvalues[results.pvalues < 0.05]\n",
    "print(\"\\nVariabili significative (p-value < 0.05):\")\n",
    "print(significant_vars)\n",
    "\n",
    "insignificant_vars = results.pvalues[results.pvalues >= 0.05]\n",
    "print(\"\\nVariabili insignificanti (p-value >= 0.05):\")\n",
    "print(insignificant_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75939c-5105-4d45-9370-c062041f1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting goodness of fit\n",
    "print(\"R2\", results.rsquared)\n",
    "print(\"RSE\", np.sqrt(results.scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602f8ff-9ee8-409a-83dd-987081d3b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_var = terms.drop(['Season', 'Round', 'Track_Temp_C', 'Humidity_%', 'Stint', 'Stint Length'])\n",
    "Xma = MS(minus_var).fit_transform(f1)\n",
    "model1 = sm.OLS(y, Xma)\n",
    "results1 = model1.fit()\n",
    "summarize(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba91e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_vars = results1.pvalues[results1.pvalues < 0.05]\n",
    "print(\"\\nVariabili significative (p-value < 0.05):\")\n",
    "print(significant_vars)\n",
    "\n",
    "insignificant_vars = results1.pvalues[results1.pvalues >= 0.05]\n",
    "print(\"\\nVariabili insignificanti (p-value >= 0.05):\")\n",
    "print(insignificant_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb17aa-f4bf-4669-91eb-c8d578835005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2\", results1.rsquared)\n",
    "print(\"RSE\", np.sqrt(results1.scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d759a",
   "metadata": {},
   "source": [
    "## Interaction Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to introduce and interactions terms use a python tuple\n",
    "X = MS(['lstat', 'age', ('lstat', 'age')]).fit_transform(Boston)\n",
    "model2 = sm.OLS(y, X)\n",
    "summarize(model2.fit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6dc96",
   "metadata": {},
   "source": [
    "## Non-linear Transformations of the Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly() function present in the package ISLP specifies that columns representing polynomial functions of its first argument are added to the model matrix\n",
    "X = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston)\n",
    "model3 = sm.OLS(y, X)\n",
    "results3 = model3.fit()\n",
    "summarize(results3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4fca2",
   "metadata": {},
   "source": [
    "## Qualitative predictors\n",
    "Based on the Carseats dataset present in the package ISLP, will attempt to predict Sales (child car seat sales) in 400 locations based\n",
    "on a number of predictors.\n",
    "\n",
    "The Carseats data includes the qualitative predictor \"ShelveLoc,\" which indicates the quality of the shelving location with three possible values: Bad, Medium, and Good.\n",
    "\n",
    "In general for qualitative predictor the ModelSpec() generates one-hot encoding of the categorical variables automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c816f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Carseats = load_data('Carseats')\n",
    "Carseats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "allvars = list(Carseats.columns.drop('Sales'))\n",
    "y = Carseats['Sales']\n",
    "final = allvars + [('Income', 'Advertising'),\n",
    "('Price', 'Age')]\n",
    "X = MS(final).fit_transform(Carseats)\n",
    "model = sm.OLS(y, X)\n",
    "summarize(model.fit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c65f5",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399f6e1",
   "metadata": {},
   "source": [
    "### Validation set approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a8685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.model_selection import (cross_validate ,KFold ,ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm #a wrapper that enables us to easily use the cross-validation tools of sklearn with models fit by statsmodels\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec999785",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')\n",
    "Auto_train , Auto_valid = train_test_split(Auto, test_size=196, random_state=0) # random_state is needed for reproducible result across run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfdc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a linear regression model\n",
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train , X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99720fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model using the MSE on the validation data\n",
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691437a1",
   "metadata": {},
   "source": [
    "## Leave One-out Cross validation\n",
    "The sklearn_sm() class takes a statsmodels model as its first argument. It also accepts two optional arguments: model_str for specifying a formula, and model_args, which is a dictionary containing additional arguments for fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_model = sklearn_sm(sm.OLS, MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "\n",
    "# This is a LOOCV because cv use the number of sample in our dataset, that is Auto.shape[0], making sure that each sample is use as a test set\n",
    "cv_results = cross_validate(hp_model ,X, Y, cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479be22",
   "metadata": {},
   "source": [
    "## K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_model = sklearn_sm(sm.OLS, MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "cv_results = cross_validate(hp_model, X, Y, cv=kf)\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f5340",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Train and compare the performance of different cross-validation methods to identify the best model for polynomial regression with varying degrees using the Auto dataset.\n",
    "\n",
    "https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077f14c",
   "metadata": {},
   "source": [
    "Fine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StatLearning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
